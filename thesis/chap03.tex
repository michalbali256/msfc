\chapter{Simulation}
\label{chap3}

As discussed in previous chapter, we implemented MSFC in ns-3 to see how well it performs in simulated conditions and compare it to other traffic schedulers for the benchmark. We simulate a part of Wi-Fi network infrastructure similar to networks of wireless-based Internet providers, which is the environment targeted by the scheduler. We compare the MSFC to 3 other traffic schedulers: CoDel, FQ CoDel and pfifo\_fast. In each run, we install the benchmarked scheduler on all network interfaces of all nodes in the network.


\section{Simulation testbed}
\label{testbed}
\begin{figure}
	\centering
	\includegraphics[width=137mm]{drawings/layout}
	\caption{The simulated network topology}
	\label{fig11:sim_layout}
\end{figure}


The simulated network is illustrated in Figure \ref{fig11:sim_layout}. The network is tree-shaped. The topmost node is server that serves as the rest of the Internet. Further, there is part of the infrastructure of an ISP. There are gateway nodes G1-G7. All gateways have 2--5 children. The leafs of this tree are access points (APs) of wireless networks. Finally, 8--12 clients (customers of ISP) are connected to each AP, this results in total of 140 clients.

Each client is connected to exactly one AP. The clients are connected with 802.11ac Wi-Fi --- they share the Wi-Fi bandwidth. APs and gateways are connected with point-to-point links with 100Mbps bandwidth and 5ms delay. The server is connected to the root node G1 of the tree using 1000Mbps link with 50ms delay.

The clients download and upload data. All the traffic flows between the server and the clients. There are several types of flows that model various behaviours of real-world Internet users. The types are listed in table \ref{tab:traffic}. They vary in transport protocol used, size of a single packet and the data rate at which they generate traffic. The data rate may be constant --- in that case the application generates packets on regular basis (e.g. it sends a packet every 5 milliseconds), or it may be variable.

The applications with variable bit rate turn on and off on irregular basis. When switched off, it does not send any packets and when switched on it sends packets at configured constant rate. The on and off durations are generated randomly --- using normal distribution $\mathcal{N}(1,1)$ bound to interval~$\left\langle0,2\right\rangle$.

The count ratio column specifies the ratio of number of applications installed per type. The values in the Table \ref{tab:traffic} mean, that there are 20 times more \emph{HTTP} flows than \emph{SSH} flows. We set the total number of applications to 280 --- 2 applications are installed on every client. 

\begin{table}
	\centering
	
	\begin{tabular}{@{}lllllll@{}}
		\toprule
		Name     & Protocol & Data rate & C/VBR & directions & Packet  & Count \\
		         &          &           &       &            & size(B) & ratio \\ \midrule
		SSH      & TCP      & 1 kbps    & CBR   & both       & 20      & 1     \\
		VoIP     & TCP      & 60 kbps   & CBR   & both       & 208     & 1     \\
		Game     & TCP      & 100 kbps  & CBR   & both       & 512     & 1     \\
		TV       & TCP      & 3 Mbps    & CBR   & down only  & 1450    & 3     \\
		HTTP     & TCP      & unlimited & VBR   & down only  & 256     & 20    \\
		Download & TCP      & unlimited & CBR   & down only  & 1450    & 5     \\
		torrent  & UDP      & unlimited & CBR   & down only  & 1450    & 3     \\ \bottomrule
	\end{tabular}
	\caption{Types of flows used in the simulation}
	\label{tab:traffic}
\end{table}

\begin{table}[]
	\centering
	\begin{tabular}{@{}llll@{}}
		\toprule
		             & CoDel  & FQ CoDel & MSFC   \\ \midrule
		\X{Target}   & 5 ms   & 5 ms     & 5 ms   \\
		\X{Interval} & 100 ms & 100 ms   & 100 ms \\
		\X{Flows}    & n/a    & 1024     & 1024   \\
		\X{Quantum}  & n/a    & MTU      & MTU    \\
		\X{Ratio}    & n/a    & n/a      & 2      \\ \bottomrule
	\end{tabular}
	\caption{The traffic scheduler parameters used in the simulation.}
	\label{tab:parameters}
\end{table}

In the benchmark, we ran multiple simulations with different schedulers. Each time, we installed the evaluated scheduler to all interfaces (NetDevices) of all nodes in the simulation. We tested PfifoFast, CoDel, FQ CoDel and MSFC. All the schedulers were used with the default ns-3 parameters, which are listed in Table \ref{tab:parameters}. We configured the duration of the simulation to 100 seconds.

We measured throughput, packet loss, delay and jitter using ns-3 module FlowMonitor \cite{flowMonitor}. Throughput is the rate at which packets flow through a node measured in bytes per second. Delay is the time taken to transmit a packet from sender to receiver. Jitter is the variation of delay. FlowMonitor approximates the jitter of a packet using only the delay of the previous packet:
\[
	\textsc{Jitter}(P_N) = \abs{\textsc{Delay}(P_N) - \textsc{Delay}(P_{N-1})},
\]
where $P_N$ is the n-th received packet.

\input{tab_flows_count_A.tex}

In order to prioritize traffic, MSFC needs to classify each packet to a class. We used prioritization based on DSCP field of the IP header. In each simulation, we assigned the same priority to all flows of the same type. The Table \ref{tab:flows_count_A} shows the prioritization as well as number of flows of individual types, which is the result of the flow priorities, the count ratio from Table \ref{tab:traffic} and maximum number of flows equal to 280. 

We only present results of `download' direction --- the upload direction does not have enough throughput, and since we use full-duplex point-to-point links, the results are not interesting.

\section{Simulation Results}

\input{simulation_A.tex} 

The overall results of simulation A are shown in Table \ref{tab:results_A}. More detailed information is in additional tables: Table \ref{tab:delay_A} shows average delay of types of flows, Table \ref{tab:loss_A} shows number of lost packets and Table \ref{tab:throughput_A} shows how much throughput an average flow of particular type received.

As seen in the tables the throughput of the \emph{torrent} flows in one--queue schedulers CoDel and pfifo\_fast is dramatically higher than FQ CoDel and MSFC. That naturally results in all other flows receiving less throughput, since all schedulers manage to utilize the links similarly (see throughput row in the Table \ref{tab:results_A}). The reason is that UDP does not have any congestion control and thus sends as much packets as possible regardless of being dropped. CoDel and pfifo\_fast mix all the packets in one queue so the TCP flows notice the congestion and slow down. The result is that misbehaving users (e.g. users, that ignore the signals of ongoing congestion and do not slow down) are actually advantaged by the setup. FQ CoDel and MSFC employ fair queueing principles (see \autoref{sec:fair_queueing}) that isolate the misbehaving users and thus reduce the throughput of misbehaving flows.

The Tables \ref{tab:delay_A} and \ref{tab:loss_A} with delay and loss statistics confirm the same. CoDel and pfifo\_fast have higher delay of all flows, while FQ CoDel and MSFC manage to keep low delay of all TCP flows and isolate the UDP flows. 

Additionally, because the average of packet delay may be too generalising, we present its distribution in Figure \ref{fig:overall_delay}. Here we can see that arithmetic mean does not represent the delay well, because there is a fraction of packets (less than 7\%) of the \emph{torrent} type in FQ CoDel, CoDel and MSFC that have delay over 3 seconds. The Figure \ref{fig:torrent_delay} shows the distribution of \emph{torrent} packets to illustrate the extreme delay.

The measured jitter is close to zero in all flows (see Table \ref{tab:results_A}). The jitter of all types of flows was negligible --- always less than 6\% of delay.

The most important result is that MSFC responds to the assigned priorities well. \emph{SSH}, \emph{VoIP} and \emph{game} flows achieved almost the same quality of service. Not surprisingly, since we assigned the highest priority to \emph{TV} flows, average \emph{TV} flow received 3185 kbps with MSFC, which is much more than 1327 kbps it got from FQ CoDel. Of course, the rest of flows with lesser priorities had less throughput, but the decrease again scaled with the priorities.

Figure \ref{fig:delay_flows_A} shows detailed comparison of delays of FQ CoDel and MSFC (we do not compare CoDel and pfifo\_fast, since their delay was much higher). The delay scales with the priorities: The delay of \emph{VoIP} flows is virtually the same. The delay of \emph{TV} has been reduced a bit in MSFC, because we assigned it the highest priority. \emph{HTTP} and \emph{Download} delay are a bit worse, because we assigned them lower priorities.


\begin{figure}
	\centering
	\includegraphics[width=137mm]{drawings/overall-delay-down}
	\caption{The distribution of delay in seconds. Packets with delay higher than 0.4 seconds are omitted in the distribution, but the means are computed from all values. This restriction results in displaying 93\% of data. However, the few packets have such high delay, that it considerably affects the arithmetic means. }
	\label{fig:overall_delay}
\end{figure}

\begin{figure}
	\centering
	\includegraphics[width=137mm]{drawings/type6-delay-down_A}
	\caption{The distribution of delay of \emph{torrent} packets in seconds.}
	\label{fig:torrent_delay}
\end{figure}



\begin{figure*}
	\centering
	\begin{subfigure}[b]{0.475\textwidth}
		\centering
		\includegraphics[width=\textwidth]{drawings/type1-delay-down_A}
		\caption[]%
		{{\small Delay of \emph{VoIP} flows}}    
		\label{fig:delay_voip}
	\end{subfigure}
	\hfill
	\begin{subfigure}[b]{0.475\textwidth}  
		\centering 
		\includegraphics[width=\textwidth]{drawings/type3-delay-down_A}
		\caption[]%
		{{\small Delay of \emph{TV} flows}}    
		\label{fig:delay_tv}
	\end{subfigure}

	\begin{subfigure}[b]{0.475\textwidth}   
		\centering 
		\includegraphics[width=\textwidth]{drawings/type4-delay-down_A}
		\caption[]%
		{{\small Delay of \emph{HTTP} flows}}    
		\label{fig:delay_http}
	\end{subfigure}
	\quad
	\begin{subfigure}[b]{0.475\textwidth}   
		\centering 
		\includegraphics[width=\textwidth]{drawings/type5-delay-down_A}
		\caption[]%
		{{\small Delay of \emph{download} flows}}    
		\label{fig:delay_download}
	\end{subfigure}
	\caption[]
	{\small The distribution of delay of different types of flows. We omit a small fraction of extreme values to get better visualization, however the means are calculated from all values. The quantization observable especially on MSFC delay on TV flows is caused by the fact that queue sizes (that cause the delay) are always multiples of a relatively large packet size.} 
	\label{fig:delay_flows_A}
\end{figure*}



\clearpage
\section{Traffic distribution caveat}

\input{tab_flows_count_B}

We ran a simulation with priorities according to Table \ref{tab:flows_count_B}. The prioritization demonstrates wrong configuration of MSFC --- the lowest priority is assigned to very few flows. The configuration in fact indicates, that \emph{most} of the traffic should be prioritized. However, that is impossible by design.

The notable difference is noticeable in the throughput of \emph{torrent} flows, see Table \ref{tab:throughput_B}. With this priority assignment, MSFC gave \emph{torrent} almost two times higher throughput than FQ CoDel, although we assigned the lowest priority to the \emph{torrent} flows. All the priority 1 flows have less throughput and worse delay. This unpleasant behaviour is caused by the distribution of flows into the priorities (see Table \ref{tab:flows_count_B}) and the fact, that MSFC treats flows of different priority classes absolutely separately. There are too many flows, that 'fight' for the bandwidth of priority 1 class, while there are few flows, for which MSFC reserves whole priority 0 class.

Figure \ref{fig:delay_flows_B} shows the distribution of delays delays of FQ CoDel and MSFC in this configuration.

The priority 2 flows results ended up without any notable difference. Also it is worth mentioning that even though we assigned flows with low data rate to the priority 2 class, which reserves huge bandwidth, it did not affect the overall throughput negatively. When packets of the highest priority class were not available, its bandwidth was assigned to the rest of priority classes.


\input{simulation_B.tex}


\begin{figure*}
	\centering
	\begin{subfigure}[b]{0.475\textwidth}
		\centering
		\includegraphics[width=\textwidth]{drawings/type2-delay-down_B}
		\caption[]%
		{{\small Delay of \emph{game} flows}}    
		\label{fig:delay_voip_A}
	\end{subfigure}
	\hfill
	\begin{subfigure}[b]{0.475\textwidth}  
		\centering 
		\includegraphics[width=\textwidth]{drawings/type3-delay-down_B}
		\caption[]%
		{{\small Delay of \emph{TV} flows}}    
		\label{fig:delay_tv_B}
	\end{subfigure}
	\par\bigskip % force a bit of vertical whitespace
	\begin{subfigure}[b]{0.475\textwidth}   
		\centering 
		\includegraphics[width=\textwidth]{drawings/type4-delay-down_B}
		\caption[]%
		{{\small Delay of \emph{HTTP} flows}}    
		\label{fig:delay_http_B}
	\end{subfigure}
	\quad
	\begin{subfigure}[b]{0.475\textwidth}   
		\centering 
		\includegraphics[width=\textwidth]{drawings/type5-delay-down_B}
		\caption[]%
		{{\small Delay of \emph{download} flows}}    
		\label{fig:delay_download_B}
	\end{subfigure}
	\caption[]
	{\small The distribution of delay of different types of flows in simulation, that demonstrates wrong configuration of priorities. We omit few extreme values to get better visualization, however the means are calculated from all values.} 
	\label{fig:delay_flows_B}
\end{figure*}



\section{Discussion}

We showed that MSFC can be easily used to prioritize important traffic and services that have high requirements on network resources. The results of simulation A (see Table \ref{tab:throughput_A}) showed, that the prioritized \emph{TV} flows received much more bandwidth, while maintaining other flows' quality of service reasonably. In reality, this can be the difference between working and unusable internet TV. Additionally, Table \ref{tab:loss_A} shows, that MSFC managed to eradicated packet loss of \emph{TV} (with MSFC, a total of 2 packets were dropped). 

We also presented unintuitive behaviour of MSFC under certain conditions. By design, MSFC reserves certain share of bandwidth to each priority class, regardless of number of flows in each class. This works well one way --- the more precious services are guaranteed to get certain bandwidth, as long as they are relatively rare. In contrast, puting most flows in a higher-priority class effectively causes it to become a `bloated' low-priority class where the services do not prioritize well against each other; moreover, less important or misbehaving flows in the lower priority classes will counter-intuitively receive more bandwidth, because of bandwidth ratio separation in the first layer of MSFC design.

However, with this in mind, MSFC flow classification can still be configured correctly and easily using only a rough estimation of number of flows of different types. Another possibility would be to use more priority classes, so the lowest priority class gets even less bandwidth. Note that other traffic schedulers that allocate bandwidth show similar corner cases with unintuitive behaviour --- one such case can be seen in HFSC \cite[Corner cases section]{hfscMan}.