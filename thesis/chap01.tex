\chapter{Traffic scheduling}
\label{chap1}

Devices use packet buffering due to the asynchronous nature of the data transfer in packet-switched networks. In general, it is hard to tell how many packets should a device hold in the buffer at a moment: too small buffers result in big packet loss and too big buffers result in delay increase. 

A traffic scheduler manages the packets that are stored in the buffer. Most of all, traffic scheduler chooses which packet is going to be transmitted next from the packets that are buffered in the device. Additionally, a traffic scheduler may decide to drop a packet --- to delete it. Each traffic scheduler has to handle two basic routines: enqueue and dequeue. Enqueue adds a packet to the scheduler and dequeue returns the next packet to transmit.

The natural simplest traffic scheduler is a FIFO queue --- the packets are transmitted by the order of their arrival. In traffic scheduling context, the queue is often referred to as first-come first-serve (FCFS) queue. However, as shown throughout this chapter, problems arise from using such simple solution, that must be solved by using more sophisticated methods instead.

Traffic scheduling severely affects overall performance of networks and particularly the Internet. Because of the universal Internet availability, ever-rising number of services now utilizes the internet protocol, instead of using traditional transfer media (e.g. IPTV, VoIP, teleconferencing). This results in more pressure on maintaining acceptable quality of service (QoS).

The term quality of service is used to denote overall performance of the Internet. There are a few measurable qualities, that are considered to represent QoS well. They are also used to measure performance of traffic schedulers:
\begin{itemize}
	\item Throughput --- the bit rate --- number of bits that are transmitted per second
	\item Packet loss --- ratio of packets that never reach their destination to all packets.
	\item Delay --- time that elapses between packet sending and receiving.
	\item Jitter --- variance in delay of consecutive packets.
\end{itemize}

Various qualities may be required for different services. Voice over Internet Protocol (VoIP), which allows users to to phone each other over the IP requires small packet delay, packet loss and jitter. Internet protocol television (IPTV) additionally requires sufficient throughput. On the other hand, e-mail does have little requirements. Table \ref{tab:QoS} shows different services and their QoS requirements \cite{Tanenbaum:2002:CN:572404}.

\begin{table}
	\caption{Stringency of services’ quality-of-service requirements.}
	
	\label{tab:QoS}
	\centering
	\resizebox{\columnwidth}{!}{%
		\begin{tabular}{@{}lllll@{}}
			\toprule
			Service                        & Throughput & Delay  & Jitter & Loss   \\ \midrule
			E-mail                         & Low        & Low    & Low    & Medium \\
			File sharing (e.g. FTP)        & High       & Low    & Low    & Medium \\
			Web access (e.g. HTTP)         & Medium     & Medium & Low    & Medium \\
			Remote login (e.g. SSH)        & Low        & Medium & Medium & Medium \\
			Audio on demand (e.g. Spotify) & Low        & Low    & High   & Low    \\
			Video on demand (e.g. YouTube) & High       & Low    & High   & Low    \\
			Telephony (e.g. VoIP)          & Low        & High   & High   & Low    \\
			Videoconferencing (e.g. Skype) & High       & High   & High   & Low    \\ \bottomrule
		\end{tabular}
	}
\end{table}


There is a wide range of requirements an ideal traffic scheduler should satisfy. These include avoiding poor QoS and enforcing fairness for all applications (users). Also, Internet service providers use scheduling to define the shape of traffic provided to customers. This chapter offers a summary of basic concepts used in the traffic scheduling.

\section{Bufferbloat}
\label{chap:bb}

In gateways of packet-switched networks, short-term differences between arrival and departure rate of network traffic occur naturally as a result of network multiplexing and applications' behaviour. To balance these bursts of incoming traffic, gateways use buffers --- a limited amount of memory, that stores packets that were received and wait to be forwarded by another link. In the following paragraphs we demonstrate the negative effects of using traffic scheduler as simple as FIFO queue.


If the buffer in a device is big enough, every time an outgoing link asks for the next packet, there in one available in the device regardles of differences in packets arrival and departure rate. That is desirable, because the throughput is maximized. Without buffers, the gateway has no place to store incoming packets and thus has to drop them, which decreases throughput.

Even though it may seem that bigger buffers are better, it is not true --- the more packets are in a queue, the longer packets stay in it and the longer it takes to be forwarded, which increases delay. Unfortunately, queues in modern networks tend to fill up and stay 'bloated' \cite{Gettys:2012:BDB:2063176.2063196} --- some amount of packets always stays in the queue (it never becomes empty), even if there are no incoming bursts to balance. This negative phenomenon of constantly full buffers is referred to as bufferbloat. It occupies precious space in buffers and pointlessly increases delay of packets.

\begin{figure}
	\centering
	\includegraphics[width=137mm]{drawings/tcp_no_bottleneck}
	\caption{TCP without bottleneck. The grey rectangles are packets. Horizontal dimension is time, vertical is bandwidth. That means the area of rectangle is size of packet.}
	\label{fig01:no_bottle}
\end{figure}

To understand why the queues become bloated, let us take a look at Transmission Control Protocol (TCP) \cite{rfc793}, which is currently the most widely used layer 4 protocol. It achieves the reliability of packet delivery with the following policy: After a packet is transmitted and received, the receiver sends an ACK packet back to sender, to let him know which packets were successfully received, and which packets must be resent. Waiting for an ACK after each packet would provide miserable throughput since most of the time both sides would wait a for single packet. Instead, the sender goes ahead and transmits packets without getting ACKs on previous ones. This fills the whole route (see Figure \ref{fig01:no_bottle}), so at all times one packet is being sent, and one packet is being received. The ACKs that come back retain the same spacing.

Figure \ref{fig01:no_bottle} shows connection with constant bandwidth along whole path, which is, unfortunately, rarely the case in today Internet. Typical path between the communicants consists of many hops with links of different bandwidth. One of the links will inevitably become a bottleneck --- a link with lowest bandwidth. In the resulting situation, if the sender would transmit at the rate of the adjacent link, the network would become congested. The bottleneck link would not be able to forward all the incoming traffic, its buffer would fill, and the rest of the arriving packets would be dropped. That would actually induce even more traffic, because the sender would try to resend the dropped packets.

To avoid this problem, TCP was designed to limit the upstream rate. In ideal case, it would transmit at the rate of the slowest link. However, the sender does not have any information about bottleneck. To approximate this rate, TCP uses a congestion window approach: there is an amount of bytes TCP can send without receiving an ACK. When an ACK is received, it frees space in congestion window, and TCP may permit sending the next packet, filling the window again. The time it takes to transmit a packet through path sender-receiver-sender is called RTT; this time can be also approximated from the time between sending the data and rcving ACK.

If the sender does not receive an ACK of a packet for certain amount of time referred to as timeout, the packet is considered lost and the sender resends it.

To maximize throughput, size of congestion window should be at least the (bottleneck) bandwidth-delay product (product of bandwidth and RTT) bytes. In that case, the sender fills the pipe with packets just before it receives the first ACK and is allowed to send the following packets. On the other hand, if the congestion window exceeds the bandwidth-delay product, the excessive packets stay in queues along the path and cause delay. 

One of reasons of bufferbloat is mismatch between the size of the congestion window and actual RTT \cite{CoDel}. In reality, estimating the window size is difficult: always-changing network load affects both RTT and bandwidth and paths change due to rerouting.

\begin{figure}
	\centering
	\includegraphics[width=137mm]{drawings/tcp_bottleneck_1}
	\caption{Start of a TCP communication.}
	
	\label{fig02:bottle_1}
\end{figure}


We will take a look at an example to demonstrate that we can observe bufferbloat even in simple situations. Figure \ref{fig02:bottle_1} shows a starting TCP communication. The illustrated network consists of 3 subnetworks. The left and the right have bandwidth 6 Mbps, the middle one has 2 Mbps. One packet has 750 B. The TCP Congestion window is set to the size of 20 packets. The sender starts by transmitting whole window of packets which arrive at the edge of the bottleneck network and get enqueued there, because of the bandwidth difference between left and middle subnetworks. After continuing to the right subnetwork, the packets retain spacing given by the bottleneck, as shown in Figure \ref{fig03:bottle_2}. The receiver turns incoming packets into ACKs with the same spacing. 

This way, after one RTT, TCP approximated the bandwidth of the bottleneck and the whole connection gets into state of equilibrium --- the sender sends one packet of data for each ACK it gets. The bottleneck is fully utilized --- the throughput is as high as possible. However, the buffer between the left and the middle network is never empty, which pointlessly increases RTT.

\begin{figure}
	\centering
	\includegraphics[width=137mm]{drawings/tcp_bottleneck_2}
	\caption{TCP after one RTT}
	
	\label{fig03:bottle_2}
\end{figure}

The reason why the packets stay in the buffer in Figure \ref{fig03:bottle_2} is that the congestion window is set to 5 more packets than the bandwidth-delay product. Determining the size of the congestion window is not an easy task. At start of TCP communication, slow start algorithm \cite{Jacobson:1988:CAC:52324.52356} is commonly used to determine the size of congestion window. The algorithm sets the congestion window to grow exponentially, increasing size of the window with each ACK received, until a threshold is reached or a packet is dropped. After that, congestion avoidance continues to maintain it. Over the years of TCP in service, many variants were introduced. Currently, CUBIC \cite{CUBIC} is used widely. 

The congestion windows are managed by the TCP endpoints, while congestion and bufferbloat takes place in gateways along the path. In an ideal world, the gateways on the path of a conversation could communicate with the endpoints directly in order to notify them of congestion instead of dropping packets.

Explicit Congestion Notification (ECN) \cite{rfc3168:ECN} is a protocol used by routers along network communication path to notify the endpoints that they should slow down, because there is ongoing congestion. There are 2 bits reserved for ECN directly in the IP header. It gives the routers ability to notify endpoints by marking packets instead of dropping them. Both sides have to support ECN --- the receiver has to read the 2 bits and send ACK with the same bits back. The sender then may react like the ACK does not arrive at all. In May 2017, 70 \% of popular websites provided passive support for ECN\cite{ECN:proceedings}.

This all indicates that simple tail drop queues that drop packets only when they are full may be suboptimal for internet usage, and can be superseded by more sophisticated queueing algorithms in the gateways. The algorithms involve counter-intuitive idea of dropping perfectly good packets even if buffer is not full yet. The drop indicates a problem to the endpoints that may start to optimize the upstream rate, while still having some room for balancing bursts. This approach is called Active Queue Management (AQM), and is recommended to use throughout the Internet. In the following sections, we describe AQM algorithms and concepts.

\subsection{Random Early Detection}

\begin{figure}
	\centering
	\includegraphics[width=137mm]{drawings/RED}
	\caption{The Random Early Drop algorithm (Image taken from wikipedia \cite{RED:picture}) }
	
	\label{fig04:RED}
\end{figure}

In 1993, Sally Floyd and Van Jacobson introduced Random Early Drop (RED) \cite{Floyd:1993:RED:169931.169935}. It monitored the average length of queue. Based on the average, it may drop (mark) incoming packet with certain (possibly 100 \%) probability, which is function of average. It addressed congestion avoidance problem as well as TCP synchronization problem, that were encountered using tail drop. However, it had quite a few parameters, which have to be set differently in various networks. Without this tuning, it functioned poorly, which led to general reluctance of deployment, although it was recommended by Internet Engineering Task Force \cite{rfc2309} in 1998.

To determine the average of time packets spend in queue, RED uses exponential weighted moving average:
\[
\text{\emph{avg}} := (1 - \textsc{w}_q)\text{\emph{avg}}+\textsc{w}_qq.
\]
$q$ is number of packets in the queue and $\textsc{w}_q$ is parameter of RED that represents degree of weighting decrease, or how much the average responds to new packets and how much weight have the past states. Too high $\textsc{w}_q$ would mean bias against short bursts. With lower $\textsc{w}_q$, the average is more fluid and queue responds to congestion slower.

At each enqueue, the \emph{avg} is compared to two parameters \textsc{MinThres} and \textsc{MaxThres}, as shows Figure \ref{fig04:RED}. If it is lower than \textsc{MinThres}, packet is just enqueued as is. If $avg$ is higher than \textsc{MaxThres}, the packet is marked (dropped). This ensures that if endpoints respond to the marking properly, or packets are actually dropped, the number of packets in queue will not exceed the maximum for long.

If $avg$ is between the thresholds \textsc{MinThres} and \textsc{MaxThres}, the packet is marked (dropped) with probability $p_a$, that is function of the average and number of packets since the last drop. Let $p_b$ be linear function of $avg$ that varies from 0 to $\textsc{Max}_p$ ($\textsc{Max}_\textsc{p}$ is parameter of RED):
\[
  p_b = \textsc{Max}_\textsc{p} \frac{(avg - \textsc{MinThres})}{\textsc{MaxThres} - \textsc{MinThres}}.
\]
Further, the final marking probability $p_a$ depends on when was the last packet marked (dropped):
\[
p_a = \frac{p_b}{1-count \cdot p_b},
\]
where $count$ is number of packets that were enqueued since the last mark (drop). This ensures, that dropped packets will never be too far, nor too close to each other\cite[Section 7]{Floyd:1993:RED:169931.169935}.

RED has also the option to work based on number of bytes in the buffer instead the number of packets. In this case:
\begin{align*}
p_x &= \textsc{Max}_\textsc{p} \frac{(avg - \textsc{MinThres})}{\textsc{MaxThres} - \textsc{MinThres}}.\\
p_b &= p_x \frac{\text{\emph{size}}_{packet}}{\text{\emph{size}}_{max}},\\
p_a &= \frac{p_b}{1-\text{\emph{count}} \cdot p_b},
\end{align*}

where $\text{\emph{size}}_{packet}$ is size of the packet being enqueued and $\text{\emph{size}}_{\textsc{max}}$ is maximum size of packet. This way, large packets are more likely to get marked, and the probability corresponds more precisely to actual time that the packets spend in the queue.

Over the years, several variants of RED have been introduced that we do not discuss here. With Weighted RED, different packets have different probability functions for different classes of traffic (classified for example by DSCP). Adaptive RED \cite{Floyd01adaptivered:} tunes the RED algorithm to remove sensitivity to some of the parameters. Robust RED \cite{RRED} was proposed to counter low-rate Denial-of-Service attacks.


\subsection{CoDel}
\label{CoDel}

\algnewcommand\And{}
\begin{algorithm}[p]
	\caption{CoDel algorithm}
	\label{alg:CoDel}
	\begin{algorithmic}
		\Function{controlLaw}{time, count}
			\State \Return time + \textsc{Interval} / sqrt(count)
		\EndFunction
		\Function{enqueue}{pkt}
			\State pkt.timestamp $\leftarrow$ clock()
			\State push(queue, p)
		\EndFunction
		\Function{dequeue}{}
			\State now $\leftarrow$ clock()
			\State pkt $\leftarrow$ pop(queue)
			
			\If {dropping} \Comment CoDel is in dropping state
			\If{\textbf{not} okToDrop(pkt, now)}
				\State \Comment {\parbox[t]{.8\linewidth}{ OkToDrop returns true, if sojourn time has been above \X{Target} for at least \X{Interval}.}} \vspace{3px}
				\State dropping $\leftarrow$ false \Comment Enter dropping state
			\EndIf
			\State \Comment {\parbox[t]{.8\linewidth}{ Drop the current packet and compute time for the next drop. It is possible, that the next drop should happen now, hence the while loop.}}
			\While{now \textgreater= nextDrop \textbf{and} dropping}
				\State drop(pkt)
				\State count += 1
				\State pkt $\leftarrow$ pop(queue)
				\\
				\If{\textbf{not} okToDrop(pkt, now)}
					\State dropping $\leftarrow$ false
				\Else
					\State nextDrop $\leftarrow$ controlLaw(nextDrop, count)
				\EndIf	
			\EndWhile 
		
			\ElsIf{okToDrop(pkt, now)}
				\State drop(pkt)
				\\
				\State pkt $\leftarrow$ pop(queue)
				\State dropping $\leftarrow$ true
				\State count $\leftarrow$ 1
				\State nextDrop $\leftarrow$ controlLaw(nextDrop, count)
		
			\EndIf
			\State \Return pkt
		\EndFunction
		
	\end{algorithmic}
\end{algorithm}

In 2012, Jacobson and Nichols introduced Controlled Delay (CoDel) algorithm \cite{CoDel}. Their AQM uses local minimum waiting time of packets in queue as indication of bufferbloat. It works only with sojourn time --- the time packets spend in queue. That implies it is independent of bandwidths of adjacent links --- if it is deployed in a backbone with 10 Gbps bandwidth, acceptable queue will be 6.25MB big. On the other hand, on an example slow 10 Mbps link, the corresponding acceptable buffer is only 6.25 kB. CoDel measures the time packets spend in queue and if it exceeds a threshold for a longer uninterrupted time, it starts to drop packets.

CoDel has 2 parameters:
\begin{itemize}
	\item \textsc{Target} is the target delay CoDel tries to keep.
	\item \textsc{Interval} sets period of time for which it is OK to exceed \textsc{Target}.
\end{itemize}
With these parameters, CoDel drops packets if they spend more than \X{Target} time in the queue for more than \textsc{Interval} time.

CoDel works in following way: every time a packet arrives, a time stamp is tagged to it. At dequeue, CoDel looks how long was the dequeued packet in the queue. If packets have exceeded the \X{Target} time for at least \textsc{Interval}, CoDel enters dropping state. In this state, packets are dropped at an increasing rate until the sojourn time of packets at front is lower than \X{Target}. After each drop, the next drop time is calculated as follows:
\[
  \X{DropInterval} = \frac{\textsc{Interval}}{\sqrt{n}},
\]
where $n$ is the number of packets dropped since dropping state entry and DropInterval is the time after the next packet will be dropped \footnote{CoDel does not drop packets, if fewer that MTU (Maximum Transmission Unit) worth of bytes is in queue.}. The pseudocode of the algorithm is in Algorithm \ref{alg:CoDel}.

CoDel and RED both address the bufferbloat problem and both manage to mitigate the problem at the very least. CoDel superseded RED by its easiness to deploy. However, both algorithms work above a single FIFO queue, which is their limitation especially if multiple non-cooperating traffic streams try to use this single queue at once. The next section shows how traffic scheduling may benefit from using multiple queues in parallel.

\section{Fair queueing}
\label{sec:fair_queueing}
Research shows \cite{Nagle:FQ} that simple first-come, first-serve (FCFS) scheduling does not treat all network users fairly in all situations. In many cases, we want to ensure that every user of a network is provided with the same quality of service. Further, it is beneficial if scheduling separates well and ill-behaved users, so that only the latter experience consequences of their behaviour. Set of AQM algorithms that enable this is called fair queueing.


In many of the proposed solutions, fairness is achieved by isolating different streams of packets traversing network, or flows. We use definition from \cite{Zhang:1990:VCN:99517.99525}. A flow has two properties:
\begin{itemize}
	\item A flow is a stream of packets that traverses the same route from the source to the destination and requires the same grade of service at each router or gateway in the path.
	\item In addition, every packet can be uniquely assigned to a flow using prespecified fields in the packet header.
\end{itemize}
 
\begin{figure}
	\centering
	\includegraphics[width=137mm]{drawings/parking_lot}
	\caption{The parking lot problem. G1--G3 denote gateways, F1--F4 denote traffic flows. The rest of labels are ratios of flow bandwidths.}
	
	\label{fig05:ParkingLot}
\end{figure}

One of the issues addressed by fair queueing is the parking lot problem. Typically, routers accomplish some amount of fairness by giving fair access to traffic coming from different input links. However, this approach is not fair to all flows. Consider network displayed in Figure \ref{fig05:ParkingLot}. Gateway G1 treats both flows F1 and F2 equally, thus both flows occupy half of link G1--G2. G2 does the same, it gives half of the bandwidth to flow 3 and second half to the traffic from link G1--G2. However, from perspective of flows, F1 and F2 have only quarter of the bandwidth while F3 has half of the bandwidth. In the outcoming link of G3, situation is the same and F1 only has $\frac{1}{8}$ of the bandwidth. In other words, portion of bandwidth allocated to flow drops exponentially with number of hops it gets through.

Second issue is related to behaviour of the users themselves. In gateways working on FCFS basis, misbehaving user that sends more traffic than others, and does not slow down on packet drops, may actually get bigger portion of bandwidth than well behaving users. Consider a router with several users connected on LAN, one of them is ill-behaved and generates traffic that by itself exceeds routers out--link. The rest of users detect congestion and slow down, resulting in leaving even bigger proportion of the bandwidth to the bad user. Furthermore, delay discussed in section \ref{chap:bb} is shared among all users and if one bloats the buffer, the others experience consequences too.

To battle these problems, Nagle \cite{Nagle:FQ} proposed to use one FCFS queue per one flow encountered by the gateway. At enqueue, router finds the right queue based on the IP header. Queues are dequeued in a round-robin fashion --- the queues take turns in fixed order. There is a list of all queues and at each dequeue gateway transmits the first packet of the first queue in the list. Then, the first queue is taken from the front and put in the back of the list. In other words, at each dequeue, the queue (flow) that was not served for the longest time is chosen (with the exception of brand new queues).

This solves both of mentioned problems, maintaining separate queue for each flow requires that the gateway to be able to map from flow identifiactor (for example source-destination address pair, ports and protocol may be considered too) to corresponding queue at each dequeue. This can be easily implemented in $\mathcal{O}(\log n)$, where n is number of queues, however that is not fast enough for simple routers with ever-rising bandwidths. McKenney discusses several possible implementations \cite[Section 2]{SFQ}.

Nagle's original algorithm has yet another flaw --- it doesn't take packet size into consideration. So if flow A sends only 100B packets, and flow B sends 500B packets, flow B gets five times more bandwidth. To address the flaw,  Demers et al. devised an ideal algorithm called bit-by-bit round-robin (BR) \cite{demers1989analysis}. It simulates that each queue sends one bit at a time in round-robin fashion. Based on this simulation, it computes time $t$ the whole packet would leave its queue. Then, BR inserts the packet in a queue sorted by $t$. Unfortunately, the best known algorithms that insert into sorted queue require $\mathcal{O}(\log n)$ time, where n is the number of flows (since at most one packet from each flow needs to be in the priority queue at the same time).

Although the Nagle's FQ algorithm was perfected to be truly fair, it requires too much resources. Following algorithms have $\mathcal{O}(1)$ complexity, while being only slightly less fair.


\subsection{Stochastic Fairness Queueing}


SFQ was proposed by McKenney \cite{SFQ} to address the inefficiencies of Nagle’s algorithm. It uses hashing to determine the flow each packet belongs to. Although one would normally require one queue for every flow and thus use hashing with chaining, McKenney suggests using considerably less queues and allow collision. This guarantees $\mathcal{O}(1)$ queue determination. The disadvantage is that some flows may collide, end up in the same queue, and thus be treated unfairly. However, if the number of queues is sufficiently larger than the number of active flows, probability of unfairness is low. To further beat this disadvantage, SFQ changes the hashing periodically (e.g. every 10 seconds) by varying salt of the hash function.

SFQ services its queues in round-robin fashion, without taking packet lengths into consideration, so it is unfair, if average packet size vary in the flows.

SFQ uses bufferstealing: When there is no more space in the buffer, a packet is dropped from the queue with the highest number of packets instead of dropping packet being enqueued. To implement this in $\mathcal{O}(1)$, McKenney suggests bucket sorting technique: there is an array indexed by number of packets. In $i$-th field of the array, there is a list of all queues that contain $i$ packets.

McKenney's scheme is valuable for bufferstealing and stochastic approach, however does nothing about unfairness caused by flows having different size of packets.

\subsection{Deficit round-robin}
\label{DRR}

\begin{algorithm}[t]
	\caption{DRR queueing algorithm}
	\label{alg:DRR_deq}
	\begin{algorithmic}
		\Function{enqueue}{Packet p}
		\State FID $\leftarrow$ GetFlowIdentificator(p)
		\Comment {\parbox[t]{.4\linewidth}{Gets the identificator of flow that packet belongs to. Typically, it would be the five--tuple of source and destination addresses, source and destination ports and protocol, all from IP header of the packet}}
		\State index $\leftarrow$ Hash(FID) \textbf{mod} NumberOfFlows
		\State push(Queues[index], p)
		\State \Return
		\EndFunction
		\Function{dequeue}{}
		\While{true}
		\State F $\leftarrow$ pop(ActiveQueues);
		\If {empty(F)}
		\State Deactivate(F)
		\ElsIf{F.DC \textgreater~head(F).PacketSize}
		
		\State {F.DC $\leftarrow$ F.DC - head(F).PacketSize}
		\State \Return pop(F)
		\Else \Comment{The packet is bigger than DC}
		\State F.DC $\leftarrow$ F.DC + Q
		\State push(ActiveQueues, F)
		\EndIf
		\EndWhile
		\EndFunction
	\end{algorithmic}
\end{algorithm}

Deficit round-robin \cite{EffDRR} is an algorithm that extends SFQ and takes packet length into consideration, so every flow gets the same bandwidth in the long term regardless of its packet characteristics. For each queue, it uses deficit counter that counts how many bytes were dequeued from the queue. If a packet can not be sent because it is too big, remaining bytes in the counter transfer to the next round. It still requires only $\mathcal{O}(1)$ time for all the operations.

For each flow, DRR needs parameter $Q_i$, where $i$ is flow number. $Q_i$ roughly specifies how many bytes DRR can send from flow $i$. If $Q_i = Q_j$ for all $i$ and $j$, all queues get the same share of the bandwidth, and thus is fair. If $Q_i$ is twice $Q_j$, $i$-th to $j$-th bandwidth will be in ration $2:1$.

Enqueue (pseudocode in algorithm \autoref{alg:DRR_deq}) is done in the SFQ-way: DRR chooses queue based on hash of certain IP header fields. It keeps list of active queues and works in $\mathcal{O}(1)$. When the buffer is full, packet is dropped from the fullest queue instead of dropping the incoming one.

The novelty of DRR lies in modified round-robin. Enqueue (pseudocode in algorithm \autoref{alg:DRR_deq}) works this way: there is a state variable $DC_i$ for each flow. The $DC_i$ is set to $Q_i$. At each round, DRR sends at most $DC_i$ bytes. Once sending next packet would break this rule, DRR puts queue  at the back of active queues list and sets $DC_i$ :
\[
  DC_i = DC_i - b_{sent} + Q_i.
\]
$b_{sent}$ is number of bytes sent that round from queue i --- that is subtracted from $DC_i$. Further, DRR replenishes the deficit counter, so it may send packets in the next round. If there are no packets in queue left, it is deactivated instead of putting it in the back.

\begin{algorithm}[t]
	\caption{DRR queueing algorithm}
	\label{alg:DRR_deq}
	\begin{algorithmic}
	\Function{enqueue}{Packet p}
		\State FID $\leftarrow$ GetFlowIdentificator(p)
		 \Comment {\parbox[t]{.4\linewidth}{Gets the identificator of flow that packet belongs to. Typically, it would be the five-tuple of source and destination addresses, source and destination ports and protocol, all from IP header of the packet}}
		\State index $\leftarrow$ Hash(FID) \textbf{mod} NumberOfFlows
		\State push(Queues[index], p)
		\State \Return
	\EndFunction
	\Function{dequeue}{}
		\While{true}
			\State F $\leftarrow$ pop(ActiveQueues);
			\If {empty(F)}
				\State Deactivate(F)
			\ElsIf{F.DC \textgreater~head(F).PacketSize}
				
					\State {F.DC $\leftarrow$ F.DC - head(F).PacketSize}
					\State \Return pop(F)
			\Else \Comment{The packet is bigger than DC}
				\State F.DC $\leftarrow$ F.DC + Q
				\State push(ActiveQueues, F)
			\EndIf
		\EndWhile
	\EndFunction
	\end{algorithmic}
\end{algorithm}

Although DRR may not send exactly the same amount of bytes each round, it becomes fair (if all $Q_i$ are equal) in several rounds time, because every round, all that remains in $DC_i$ from the previous round transfers to the next. Thus, any unfairness caused by atomicity of packets are smoothed over time.

\subsection{Flow Queue CoDel}

\begin{figure}
	\centering
	\includegraphics[width=137mm]{drawings/fq_codel}
	\caption{FQ-CoDel state-machine.}
	\label{fig06:fqcodel}
\end{figure}

Flow Queue CoDel \cite{fq_codel} is a traffic scheduler that combines ideas of SFQ and DRR with CoDel bufferbloat battling capability. It uses modified DRR and replaces FCFS queues for flows with CoDel queues. Currently, several linux distributions as well as systemd use FQ CoDel as the default traffic control.

In the context of FQ CoDel, term credits is used instead of deficit counter. However, the two denote the same state variable.

FQ CoDel processes the flows in the DRR fashion, however it divides active queues into two groups: new and old. The new ones are prioritized: packet is never dequeued from an old flow, if list of new queues is not empty. An empty queue becomes new, when a packet is enqueued to it. New queue becomes old (it is pushed at the back of old flows list), if it becomes empty, or its credits ($DC$) are exhausted. An old queue becomes empty, if there are no packets left in it. So every flow has its life cycle in this order: it is first empty, then new, old and empty again. The state machine is illustrated in Figure \ref{fig06:fqcodel}.

FQ CoDel has parameters \X{Target} and \X{Interval} that configure the underlying CoDels. Further, there is \X{Quantum}, that is parallel of DRR's $Q_i$, however \X{Quantum} applies to all queues. \X{Limit} is the maximum number of packets the scheduler can hold at the same time.

When a packet is enqueued in FQ CoDel, its flow is found based on its IP header in the SFQ fashion. Then, CoDel takes it over --- it is timestamped and put in the back. If the flow is empty, it is pushed back into the list of new queues and its credits are set to \X{Quantum}.

To protect buffer from overload, FQ CoDel keeps track of total packets it is holding. Since a packet is just enqueued, \X{Limit} may be exceeded. If it is, the algorithm finds queue with the largest byte count and drops 64 packets from the \textit{front}. Dropping several packets at once amortises the time needed to find the longest queue.

The algorithm does most of the work at dequeue. First, it chooses a flow $F$ to dequeue packet from --- either the head of new queues list, or the head of the old queues list, if the new one is empty. If $F$ has negative credits, it means FQ CoDel already served at least \X{Quantum} bytes, and pushes it in the back of the old queues list and starts over with choosing $F$.

Second, it dequeues packet from CoDel of flow $F$. Two things may happen:
\begin{enumerate}
	\item CoDel drops all packets on dequeue --- change state of the flow based on Figure \ref{fig06:fqcodel} and start over.
	\item CoDel returns a packet --- subtract the size of packets from credits of flow $F$ and send the packet away.
\end{enumerate}

FQ CoDel is fast and simple enough to be deployed in ordinary home routers. Furthermore, it combines good characteristics of CoDel and SFQ.

\section{Traffic shaping}

In some applications, perfectly fair queueing (the same bandwidth for all) may not be applicable. Instead, it is beneficial to give up the best-effort basis in order to maintain good QoS for all users and services in the network, as different services have different needs and different users have different service level agreements. 

Traffic shaping is set of techniques for regulating the average rate and burstiness of a flow of data that enters the network. The customer and ISP agree on certain traffic pattern (or shape) that provider then shall guarantee. ISP tries to suit all application requirements for QoS. Also, the goal is to describe traffic patterns in a simple and robust way. Traffic shaping provides ISP with the ways to do it. It is also suitable for congestion avoidance since it controls amount of traffic in network.

\subsection{Leaky bucket and Token bucket}
\label{token_bucket}
Leaky bucket (first described by Turner in \cite{turner1986new}) and token bucket are two terms used for the same concept, they only use a slightly different analogy with bucket and water/tokens as described in \cite[Section 5.4.2]{Tanenbaum:2002:CN:572404}. However, both schedule packets at constant rate and drop packets, if they flow too fast while allowing certain burstiness.

Both use the same parameters --- rate $R$, at which packets leave and size of bucket $B$, also called burst size.

\begin{figure}
	\centering
	\begin{subfigure}{.6\linewidth}
		\centering
		\includegraphics[width=75mm]{drawings/token_bucket}
		\caption{Token bucket}
		\label{fig08:token}
	\end{subfigure}%
	\begin{subfigure}{.4\linewidth}
		\centering
		\includegraphics[width=47mm]{drawings/leaky_bucket}
		\caption{Leaky bucket}
		\label{fig08:leaky}
	\end{subfigure}
	\caption{The equivalent ideas behind token bucket and leaky bucket algorithms.}
	\label{fig08:token_leaky}
\end{figure}

Illustration of a leaky bucket is in Figure \ref{fig08:leaky}. The algorithm needs a counter variable $C$. If there is any water in the bucket ($C > 0$), it leaks ($C$ decreases) at rate $R$. Every time a packet comes, its size is added to this counter. If $C + \X{PacketSize}$ is more than $B$, packet must wait until $C + \X{PacketSize} < B$ holds (until enough water leaks) or is dropped. Note, that R is not the same as the output bandwidth. When the bucket is empty, the leaky bucket sends packets as they come until the bucket is filled. 

Token bucket is in Figure \ref{fig08:token}. In this case, algorithm adds tokens to $C$ at rate $R$. Every time a packet passes, it takes tokens equal to its size from the bucket (its packet size is subtracted from $C$), or, if $C - PacketSize < 0$, the packet must wait until enough tokens are in the bucket. 

The concept may be used in two ways --- in the place of scheduler, that actually holds the packets in FCFS queue. Secondly, ISPs can use it in traffic policing, to enforce service level agreement. In that case, token/leaky bucket just manages the buckets based on packets that flow by, and drops them if the buckets are empty/full.

The fundamental feature of the bucket concept is, that it enforces certain bandwidth, while allowing short-term bursts (that are necessary in packet switching networks). Let M be maximum output rate in B/s, and t the time between start of burst and dropping (withholding) first packet. Then it holds:
\[
	B + Rt = Mt,
\]
since water leaks from the bucket even after the burst begins. $B + Rt$ is number of bytes bucket permits, $Mt$ is the number of bytes output link is able to transmit. Thus,
\[
	\X{MaxBurstSize} = Mt = M\frac{B}{M - R}.
\]

\begin{figure}
	\centering
	\includegraphics[width=110mm]{drawings/tbf}
	\caption{Token bucket filter. Picture taken from wikipedia \cite{TBF:picture}}
	
	\label{fig09:tbf}
\end{figure}

\subsection{Token Bucket Filter}
Token Bucket Filter (TBF) is a traffic scheduler (qdisc) implemented in the Linux kernel. It uses the token bucket with FCFS queue. Additionally, it limits peak maximum output rate by adding second small bucket. This bucket is only the size of the maximum transmission unit (MTU), and has a configurable peak rate --- TBF never transmits at higher rate than this peak. It prevents the scheduler from taking all the bandwidth of the output link.

The layout of TBF is shown in Figure \ref{fig09:tbf}. TBF enqueues packets into a FCFS queue. It may become full, if number of bytes in it exceeds $Limit$ --- then incoming packet is simply dropped. The dequeue is controlled by two token buckets --- both of them must have enough tokens to allow packet sending. The smaller bucket size is MTU, \X{Rate}, \X{PeakRate} as well as the size of the bigger bucket \X{Burst} are parameters of the algorithm.

\subsection{Hierarchical token bucket}
\begin{figure}
	\centering
	\includegraphics[width=137mm]{drawings/hierarchy}
	\caption{Traffic pattern organized in a hierarchy.}
	
	\label{fig10:hierarchy}
\end{figure}
In the area of traffic shaping, it is important to be able to precisely define traffic shape, and to have an algorithm that can follow this definition as precisely as possible. Hierarchic definitions are a natural way to do so. One such hierarchy is shown in Figure \ref{fig10:hierarchy}. It shows a link with 45 Mbps of bandwidth. Two organizations: CUNI and CTU share this link. The first one is guaranteed 25 Mbps and the latter 20 Mbps of the bandwidth. Both organizations define their internal traffic shape they want to have guaranteed.

Every node is guaranteed to receive \textit{at least} the specified rate at all times. Additionally, we want the link to be always utilized as much as possible. For example, when CUNI only transmits 5 Mbps of video traffic, the excess 3 Mbps may be shared with the rest of the classes --- first with sibling ones, and if whole CUNI does not generate enough traffic, CTU gets more than the guaranteed 20 Mbps.


We need an algorithm, that would enforce the hierarchy. One such algorithm is Hierarchical Token Bucket (HTB) \cite{HTB}. It divides traffic into classes that are further hierarchically grouped in a tree. The classes are divided into 8 levels of priority, which affects delay and distribution of excess resources but not the guaranteed rate. At actual dequeue, HTB uses deficit round-robin to serve all classes of the same priority, while conforming to token bucket limitations.

Every node of the tree takes several parameters:
\begin{itemize}
	\item \X{Priority} --- the classes with higher priority are served first.
	\item \X{Rate} --- the guaranteed rate the class gets.
	\item \X{Ceil} --- the maximum rate the class can get.
	\item \X{Burst} --- size of the main bucket (see \ref{token_bucket}).
	\item \X{Cburst} --- size of the ceil (peak) bucket. It should be set to a size close to the average size of packet.
\end{itemize}

There are two token buckets in each class, one with size \X{Burst} and rate \X{Rate}. The second which has size \X{Cburst} and rate \X{Ceil} determines maximum rate. The main idea of HTB that allows distribution of excess resources is that a child class can borrow tokens from its parents, if the child has exceeded \X{Rate} (and do not have any tokens left in the main bucket). Each class is, however, still limited by the second smaller bucket --- with the \X{Ceil} rate. Classes can be divided into 3 groups during the scheduler operation:
\begin{itemize}
	\item Green --- The class has enough tokens (did not exceed \X{Rate})
	\item Yellow --- The class exceeds \X{Rate}, but does not exceed \X{Ceil}. It may borrow from parent class(es).
	\item Red --- The class exceeds \X{Ceil}, and packets must be backlogged until the class gets more tokens (either its own or borrowed).
\end{itemize}


When HTB determines from which class to dequeue, first it chooses the colour of the class. It serves a yellow class only if there are no green ones. Red classes are never served. The green classes have not reached the guaranteed rate yet and thus have the highest priority.

Next, HTB filters classes based on priority: HTB never chooses class of priority $i$ if any of the priorities $\left\langle0,i\right)$ is available. If there are more classes of the same priority and color, they are served in round-robin fashion using DRR with the \X{Quantum} of classes set proportionally to the \X{Rate} of the class. By default, \X{Quantum} is $\frac{\X{rate}}{10}$. 

The yellow classes may only borrow from its parent, if it is not red. If it is yellow, it results in recursion --- the parent has to borrow tokens from grandparent and so on.

In other words, HTB uses the token bucket hierarchy to mark classes with colour and thus prioritize ones that have not received \X{Rate}. On the second level, it prioritizes classes with higher priority. On the third level, all classes are equal, thus it uses DRR. On the fourth level, there is usually FCFS queue to buffer the withheld packets. However, in the implementation in the Linux kernel, one may assign any qdisc to each class.

